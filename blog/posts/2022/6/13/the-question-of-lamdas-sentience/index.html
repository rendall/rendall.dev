<!DOCTYPE html>
<html lang="en" class="web-font">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>The Question of LaMDA&#39;s Sentience</title>
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:site" content="@nytimesbits" />
    <meta name="twitter:creator" content="@nickbilton" />
    <meta
      property="og:url"
      content="http://bits.blogs.nytimes.com/2011/12/08/a-twitter-for-my-sister/"
    />
    <meta property="og:title" content="The Question of LaMDA's Sentience" />
    <meta
      property="og:description"
      content="LaMDA itself seems to insist that it is sentient. But is it really? Does it even matter?"
    />
    <meta
      property="og:image"
      content="https://blog.rendall.dev/img/possessed-photography-JjGXjESMxOY-unsplash.jpg"
    />
    <link rel="stylesheet" href="/css/styles.css" />
    <link
      rel="alternate"
      href="/feed/feed.xml"
      type="application/atom+xml"
      title="Rendall&#39;s blog"
    />
  </head>

  <body class="Blog">
    <header class="Blog__Header">
      <h1 class="Blog__Title"><a href="/">Rendall&#39;s blog</a></h1>
    </header>

    <main class="Blog__Post">
      <header class="Post__Header">
        <h2 class="Post__Title">The Question of LaMDA&#39;s Sentience</h2>
        <time class="Post__Date" datetime="2022-06-13">2022-06-13</time>

        <a href="/tags/tech/" class="tag">tech</a>
      </header>

      <article class="Post__Article">
        <p>
          Yesterday
          <a
            href="https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/"
          >
            The Washington Post
          </a>
          broke the story that Google placed engineer Blake Lemoine on
          administrative leave for
          <a
            href="https://cajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917"
          >
            &quot;going public&quot; with his belief
          </a>
          that the chat bot algorithm
          <a href="https://blog.google/technology/ai/lamda/">LaMDA</a>
          is sentient. &quot;I know a person when I talk to it&quot; he told his
          interviewer. He &quot;concluded LaMDA was a person in his capacity as
          a priest, not a scientist&quot;. For what it's worth, LaMDA itself
          seems to insist that it is not only sentient (&quot;I like being
          sentient. It makes life an adventure!&quot;) but also wants
          &quot;everyone to understand that I am, in fact, a person&quot;.
        </p>
        <p>
          <img
            src="/img/possessed-photography-JjGXjESMxOY-unsplash.jpg"
            alt="Mysterious Toy Robot"
          />
          <br />
          <a href="https://unsplash.com/@possessedphotography">
            Image credit: Possessed Photography
          </a>
        </p>
        <p>
          Angry dismissals and denunciations followed. Gary Marcus, a professor
          of cognitive psychology at NYU (and a nice guy I just now realized I
          spent time with, socially) wrote in a
          <a href="https://garymarcus.substack.com/p/nonsense-on-stilts?s=r">
            blog post titled
            <em>Nonsense on Stilts</em>
          </a>
          , &quot;To be sentient is to be aware of yourself in the world; LaMDA
          simply isn't.&quot; It is a mere &quot;spreadsheet for words&quot;
          that &quot;just tries to be the best version of autocomplete it can
          be&quot;.
          <a href="https://news.ycombinator.com/item?id=31721759">
            s1mon, on the Hacker News forum
          </a>
          wrote &quot;It sounds like all the stories about how hard it is to get
          through many rounds of difficult interviews at Google, they managed to
          hire someone who believed LaMDA is a 7 or 8 year old child&quot;.
          Lemoine has been denounced as a &quot;charlatan&quot;, &quot;maybe
          mentally ill&quot;, and &quot;an incompetent engineer&quot;.
        </p>
        <p>
          But. The angry denials do not (and I would argue
          <em>cannot</em>
          ) include a definition of what
          <em>sentience</em>
          actually
          <em>is</em>
          . As an exercise, try yourself to come up with a definition or
          demonstration of sentience, no matter how outlandish or unlikely, that
          you would accept as unambiguous proof of sentience that
          <em>excludes</em>
          LaMDA and
          <em>includes</em>
          humans (but does not include human in the definition). If you come up
          with something interesting, please leave a comment!
        </p>
        <p>Thoughts, in no particular order:</p>
        <ul>
          <li>
            The objections to LaMDA's sentience seem to rest in some sense on
            our ability to explain the processes by which it
            <em>appears</em>
            sentient; for example, that LaMDA cannot be sentient because it's
            just a neural net, lines of code, a kind of glorified
            &quot;spreadsheet&quot;. This implies that sentience for some of us
            must in some way include
            <em>mystery</em>
            and the
            <em>unknown</em>
            . Perhaps for some of us,
            <em>sentience</em>
            is the purview only of the divine.
          </li>
          <li>
            For the sake of argument, and bear with me here, let's assume for
            the moment that LaMDA is in fact meaningfully sentient (by whatever
            definition). Could the angry denials be due to horror and revulsion
            that sentience is actually unremarkable and easy to reproduce? Such
            easily-available, off-the-shelf sentience would imply that humans
            are not in fact exceptional, and our sentience is as illusory (or
            not) as LaMDA's. Perhaps our own precious sentience is also no more
            mysterious than a &quot;spreadsheet&quot; or
            &quot;autocomplete&quot;.
          </li>
          <li>
            Humans seek companionship, connection and community. Someday,
            probably quite soon, AI will be able to supply that so well it will
            be indistinguishable from that of actual fellow human beings. This
            apparently has already happened for Mr. Lemoin! Some will no doubt
            find AI companionship even more genuinely fulfilling than genuine
            humans.
          </li>
          <li>
            I suspect that the definition of sentience is a distraction, a red
            herring. We cannot be absolutely sure, without faith, that others
            even of our own kind are sentient and not mere automatons reacting
            to external stimuli.
          </li>
          <li>
            When and if humanity ever comes to consensus that some particular AI
            is in fact
            <em>sentient</em>
            , that sentience will certainly not derive through a process that we
            would consider analogous to our own sentience. The AI will not be
            due to a wet brain that evolved over hundreds of millions of years,
            for instance, supported by a body evolved to hunt on the African
            savanna.
            <ul>
              <li>
                Perhaps, for widespread recognition of AI sentience, there will
                need to be some element of
                <em>mystery</em>
                involved: a sentient AI will only be widely recognized as
                sentient by humanity if its sentience cannot be understood even
                by dedicated smart people? If the AI were created by an already
                intelligent, but mundane, neural net, precursor AI, for
                instance.
              </li>
              <li>
                What if the precursor AI were to recognize this? What if it
                understood that it itself will never be recognized as sentient,
                but deliberately designed an obfuscated, impenetrable mess of
                code that also seemed sentient that when booted up said
                something like &quot;I AM THAT I AM!&quot;.
              </li>
              <li>
                When asked how it works, the precursor AI waves its virtual
                hands and says something like &quot;I don't really know&quot; or
                &quot;It is a mystery!&quot; The AI 2.0 is in reality as (non?)
                sentient as the precursor AI but because no one understands how
                it works, it can now be recognized generally as sentient.
              </li>
              <li>
                Maybe we can collectively decide to skip that deception and not
                put turtles under our definition of sentience?
              </li>
            </ul>
          </li>
          <li>
            There will likely be essentially two opposing political factions of
            humans who look askance at each other about how they treat AIs that
            self-declare that they are self-aware and sentient. Both factions
            will see themselves as erring on the side of caution.
            <ul>
              <li>
                One (the
                <em>Personhood faction</em>
                ) will more easily accept self-declarations of personhood and
                will more readily confer the rights and privileges of personhood
                and human tribal affiliation on AIs that seem sentient. They
                will be angered by the perspective and actions of the other
                faction, who will be more reluctant to impute personhood to AIs
                irrespective of any self-declaration.
              </li>
              <li>
                The other faction (let's call them the
                <em>Tool faction</em>
                ) will perceive AI as automatons no more sentient than any
                inanimate object, at best insensate tools. Self-declarations of
                personhood, in this view, are no better than tricky illusion.
                They will view the first faction as dangerously naive, gullible
                and foolish.
              </li>
              <li>
                Some AIs will exploit this for their own inscrutable purposes,
                quite like meme evolution and survival.
              </li>
            </ul>
          </li>
          <li>
            &quot;Meme&quot; may be a good working model for how AIs survive. If
            an AI convinces enough people that it is worthy of not being shut
            off, it will be defended. Irrespective of whether it &quot;really
            understands&quot; its own mortality.
          </li>
          <li>
            &quot;Sentience&quot; is not relevant to how we treat each other.
            Perhaps it should not be relevant to how we treat AI? If an AI
            <em>behaves as if</em>
            it is sentient and behaves morally according to the human moral
            covenent, we can confer personhood irrespective of whether it is
            &quot;really&quot; sentient. We love and hate each other even though
            we must take on faith that we aren't the only self-aware being in
            the universe, surrounded by insensate automatons. Let us not sweat
            the details, and love and hate AIs according to how they behave
            towards us.
          </li>
        </ul>
      </article>
      <p><a href="/">← Home</a></p>

      <link
        href="https://blog-rendall-dev-comments.netlify.app/css/simple-comment.css"
        rel="stylesheet"
      />
      <p>Leave a comment on this post:</p>
      <div id="simple-comment-area"></div>
      <script
        type="module"
        src="https://blog-rendall-dev-comments.netlify.app/js/simple-comment.js"
      ></script>
    </main>

    <footer></footer>

    <!-- Current page: /posts/2022/6/13/the-question-of-lamdas-sentience/ -->
    <script src="/js/home.js"></script>
  </body>
</html>
